{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mvenk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mvenk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import findspark\n",
    "findspark.init()\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, pandas_udf,col, lower, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.ml import PipelineModel, Pipeline, Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.ml.util import DefaultParamsWritable, DefaultParamsReadable\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define English stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Text Classification with PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('twitter_training.csv', header=False, inferSchema=True)\n",
    "validation = spark.read.csv('twitter_validation.csv', header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define column names\n",
    "columns = ['id', 'Company', 'Label', 'Text']\n",
    "\n",
    "# Rename columns\n",
    "for i, col in enumerate(columns):\n",
    "    data = data.withColumnRenamed('_c{}'.format(i), col)\n",
    "    validation = validation.withColumnRenamed('_c{}'.format(i), col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Text'])\n",
    "validation = validation.dropna(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Text|\n",
      "+--------------------+\n",
      "|im getting on bor...|\n",
      "|I am coming to th...|\n",
      "|im getting on bor...|\n",
      "|im coming on bord...|\n",
      "|im getting on bor...|\n",
      "|im getting into b...|\n",
      "|So I spent a few ...|\n",
      "|So I spent a coup...|\n",
      "|So I spent a few ...|\n",
      "|So I spent a few ...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data.select(\"Text\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping:\n",
      "Index 0 --> Label 'Negative'\n",
      "Index 1 --> Label 'Positive'\n",
      "Index 2 --> Label 'Neutral'\n",
      "Index 3 --> Label 'Irrelevant'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the StringIndexer for the label column (index the labels)\n",
    "label_indexer = StringIndexer(inputCol=\"Label\", outputCol=\"Label2\")\n",
    "\n",
    "# # Define your index mapping\n",
    "# class_index_mapping = { \"Negative\": 0, \"Positive\": 1, \"Neutral\": 2, \"Irrelevant\": 3 }\n",
    "\n",
    "# Fit StringIndexer on data\n",
    "label_indexer_model = label_indexer.fit(data)\n",
    "data = label_indexer_model.transform(data)\n",
    "validation = label_indexer_model.transform(validation)\n",
    "\n",
    "# Extract label mapping\n",
    "label_mapping = label_indexer_model.labels\n",
    "\n",
    "# Print label mapping\n",
    "print(\"Label Mapping:\")\n",
    "for index, label in enumerate(label_mapping):\n",
    "    print(f\"Index {index} --> Label '{label}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(df, inputCol=\"Text\", outputCol=\"cleaned_text\"):\n",
    "    # Remove links starting with https://, http://, www., or containing .com\n",
    "    df = df.withColumn(outputCol, regexp_replace(df[inputCol], r'https?://\\S+|www\\.\\S+|S+\\.com\\S+|youtu\\.be/\\S+', ''))\n",
    "    # Remove words starting with # or @\n",
    "    df = df.withColumn(outputCol, regexp_replace(df[outputCol], r'(@|#)\\w+', ''))\n",
    "    # Convert text to lowercase\n",
    "    df = df.withColumn(outputCol, lower(df[outputCol]))\n",
    "    # Remove non-alpha characters\n",
    "    df = df.withColumn(outputCol, regexp_replace(df[outputCol], r'[^a-zA-Z\\s]', ''))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_text(data, inputCol=\"Text\", outputCol=\"Text\")\n",
    "cleaned_validation = clean_text(validation, inputCol=\"Text\", outputCol=\"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"tokens\")\n",
    "\n",
    "# Define stopwords remover\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\", stopWords=stop_words)\n",
    "\n",
    "# Define CountVectorizer\n",
    "count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "# Define Logistic Regression\n",
    "lr = LogisticRegression(maxIter=10, labelCol=\"Label2\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, count_vectorizer, lr])\n",
    "\n",
    "# Apply the pipeline to the data\n",
    "model = pipeline.fit(cleaned_data)\n",
    "processed_data = model.transform(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      " |-- Label2: double (nullable = false)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+\n",
      "|                Text|Label2|prediction|\n",
      "+--------------------+------+----------+\n",
      "|im getting on bor...|   1.0|       1.0|\n",
      "|i am coming to th...|   1.0|       1.0|\n",
      "|im getting on bor...|   1.0|       1.0|\n",
      "|im coming on bord...|   1.0|       1.0|\n",
      "|im getting on bor...|   1.0|       1.0|\n",
      "|im getting into b...|   1.0|       1.0|\n",
      "|so i spent a few ...|   1.0|       1.0|\n",
      "|so i spent a coup...|   1.0|       1.0|\n",
      "|so i spent a few ...|   1.0|       1.0|\n",
      "|so i spent a few ...|   1.0|       1.0|\n",
      "| so i spent a few...|   1.0|       1.0|\n",
      "|                 was|   1.0|       0.0|\n",
      "|rockhard la varlo...|   2.0|       2.0|\n",
      "|rockhard la varlo...|   2.0|       2.0|\n",
      "|rockhard la varlo...|   2.0|       2.0|\n",
      "|rockhard la vita ...|   2.0|       2.0|\n",
      "|live rock  hard m...|   2.0|       2.0|\n",
      "|ihard like me rar...|   2.0|       2.0|\n",
      "|that was the firs...|   1.0|       1.0|\n",
      "|this was the firs...|   1.0|       1.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.select(\"Text\", \"Label2\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+----------+---------------+\n",
      "|Text                                   |prediction|Predicted_Label|\n",
      "+---------------------------------------+----------+---------------+\n",
      "|this is a great product                |1.0       |Positive       |\n",
      "|i didnt like the service               |0.0       |Negative       |\n",
      "|neutral comment about the product      |1.0       |Positive       |\n",
      "|not relevant comment that doesnt matter|3.0       |Irrelevant     |\n",
      "+---------------------------------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, create_map\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define your class index mapping as a list of tuples\n",
    "class_index_mapping = [(0, \"Negative\"), (1, \"Positive\"), (2, \"Neutral\"), (3, \"Irrelevant\")]\n",
    "\n",
    "# Create a mapping DataFrame\n",
    "mapping_df = spark.createDataFrame(class_index_mapping, [\"prediction\", \"Predicted_Label\"])\n",
    "\n",
    "# Function to clean the new input text\n",
    "def clean_new_text(df, inputCol=\"Text\", outputCol=\"cleaned_text\"):\n",
    "    \"\"\"Clean the new text data.\"\"\"\n",
    "    df = df.withColumn(outputCol, regexp_replace(df[inputCol], r'https?://\\S+|www\\.\\S+|\\.com\\S+|youtu\\.be/\\S+', ''))\n",
    "    df = df.withColumn(outputCol, regexp_replace(df[outputCol], r'(@|#)\\w+', ''))\n",
    "    df = df.withColumn(outputCol, lower(df[outputCol]))  # Convert text to lowercase\n",
    "    df = df.withColumn(outputCol, regexp_replace(df[outputCol], r'[^a-zA-Z\\s]', ''))  # Remove non-alpha characters\n",
    "    return df\n",
    "\n",
    "new_texts = [(\"Company1\", \"This is a great product!\"), \n",
    "              (\"Company2\", \"I didn't like the service.\"), \n",
    "              (\"Company3\", \"Neutral comment about the product.\"),\n",
    "              (\"Company4\", \"Not relevant comment that doesn't matter.\")]\n",
    "\n",
    "# Create a DataFrame for new texts\n",
    "new_text_df = spark.createDataFrame(new_texts, [\"Company\", \"Text\"])\n",
    "\n",
    "# Clean the new text data\n",
    "cleaned_new_text = clean_new_text(new_text_df, inputCol=\"Text\", outputCol=\"Text\")\n",
    "\n",
    "# Use the trained model to make predictions on the cleaned new text\n",
    "predictions = model.transform(cleaned_new_text)\n",
    "\n",
    "# Join predictions with the mapping DataFrame to get the corresponding labels\n",
    "predictions_with_labels = predictions.join(mapping_df, on=\"prediction\", how=\"left\")\n",
    "\n",
    "# Show the predictions with labels\n",
    "predictions_with_labels.select(\"Text\", \"prediction\", \"Predicted_Label\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
